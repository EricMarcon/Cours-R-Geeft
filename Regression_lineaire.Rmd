---
title: "Régression linéaire"
author: "Eric Marcon"
date: "`r format(Sys.time(), '%d %B %Y')`"
url: https://EricMarcon.github.io/Cours-R-Geeft/
github-repo: EricMarcon/Cours-R-Geeft
bibliography: references.bib
biblio-style: chicago
urlcolor: blue
output:
  bookdown::beamer_presentation2:
    latex_engine: xelatex
    includes:
      in_header: latex/header.tex
    citation_package: natbib
    slide_level: 2
    df_print: default
    number_sections: no
    toc: no
    fig_caption: no
    keep_tex: no
  bookdown::ioslides_presentation2:
    logo: images/logo.png
    widescreen: true
---

```{r}
#| label: DoNotModify
#| include: false
### Utilities. Do not modify.
# Installation of packages if necessary
InstallPackages <- function(Packages) {
  InstallPackage <- function(Package) {
    if (!Package %in% installed.packages()[, 1]) {
      install.packages(Package, repos = "https://cran.rstudio.com/")
    }
  }
  invisible(sapply(Packages, InstallPackage))
}

# Basic packages
InstallPackages(c("bookdown", "formatR", "kableExtra", "ragg"))

# kableExtra must be loaded 
if (knitr::opts_knit$get("rmarkdown.pandoc.to") == "docx") {
  # Word output (https://stackoverflow.com/questions/35144130/in-knitr-how-can-i-test-for-if-the-output-will-be-pdf-or-word)
  # Do not use autoformat (https://github.com/haozhu233/kableExtra/issues/308)
  options(kableExtra.auto_format = FALSE)
}
library("kableExtra")

# Chunk font size hook: allows size='small' or any valid Latex font size in chunk options
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
```

```{r}
#| label: Options
#| include: false
### Customized options for this document
# Add necessary packages here
Packages <- c(
  "tidyverse",
  "scatterplot3d",
  "AICcmodavg",
  "MASS"
)
# Install them
InstallPackages(Packages)

# knitr options
knitr::opts_chunk$set(
  cache =   TRUE,     # Cache chunk results
  include = TRUE,     # Show/Hide chunks
  echo =    TRUE,     # Show/Hide code
  warning = FALSE,    # Show/Hide warnings
  message = FALSE,    # Show/Hide messages
  # Figure alignment and size
  fig.align = 'center', out.width = '80%',
  # Graphic devices (ragg_png is better than standard png)
  dev = c("ragg_png", "pdf"),
  # Code chunk format
  tidy = FALSE, tidy.opts = list(blank = FALSE, width.cutoff = 50),
  size = "scriptsize", knitr.graphics.auto_pdf = TRUE
)
options(width = 50)

# ggplot style
library("tidyverse")
theme_set(theme_bw())
theme_update(
  panel.background = element_rect(fill = "transparent", colour = NA),
  plot.background = element_rect(fill = "transparent", colour = NA)
)
knitr::opts_chunk$set(dev.args = list(bg = "transparent"))

# Tibbles: 5 lines, fit to slide width
options(tibble.print_min = 5, tibble.width = 50)

# Random seed
set.seed(973)
```


# Régression linéaire simple

## Ventoux

Données du projet de dendrométrie 2020, Mont Ventoux.

```{r}
read_csv2("data/Inv_GEEFT_Ventoux_09-2020.csv") |> 
  rename(
    espece = Espèce, 
    diametre = `Diamètre (cm)`, 
    hauteur = `Hauteur réelle (m)`
  ) |> 
  mutate(
    espece = case_match(
      espece, 
      "P" ~ "Pin",
      "C" ~ "Cèdre"
    )
  ) -> ventoux
```


## Graphique hauteur ~ diamètre

```{r}
#| out.width: 70%
ventoux |> 
  ggplot(aes(x = diametre, y = hauteur)) +
  geom_point(aes(col = espece)) +
  geom_smooth(method = "lm")
```


## Théorie

Modèle linéaire simple :
$$Y = \beta_0 + \beta_1 X + \Epsilon$$
$Y$ et $X$ sont des vecteurs : $Y = \{y_i\}$ est l'ensemble des observations. Par abus d'écriture, $Y$ est aussi la variable aléatoire dont les $y_i$ sont des réalisations.

Vocabulaire : variable expliquée, exogène, coefficients, constante (intercept)...

$\Epsilon = \{\epsilon_i\}$ est l'erreur du modèle. $\Epsilon \sim \mathcal{N}(0,\sigma^{2})$
 

## Représentation 

Le modèle prédit une densité de probabilité des valeurs de $Y$ pour toute valeur de $X$ distribuée normalement autour de la droite de régression.

```{r}
#| echo: false
#| label: mod_l1_params
beta_0 <- 1
beta_1 <- 0.5
sigma <- 1
```


```{r}
#| echo: false
n_fig <- 20
# Jeu de points
mod_l1fig <- tibble(
  x = rnorm(n_fig, mean = 10, sd = 2), # x est calculé avant y 
  y = rnorm(n_fig, mean = beta_0 + beta_1 * x, sd = sigma) # y utilise x
)
# Points y*
x0 <- 7:13
mod_l1fig_predict <- tibble(x = x0, y = beta_0 + beta_1 * x0)
# Distribution autour de y*. Le nombre de points doit être multiple de lengh(x0)
norm_dta <- tibble(
  y = rnorm(7000, mean = beta_0 + beta_1 * x0, sd = sigma), 
  x = x0 + dnorm(x = y- beta_0 - beta_1 * x0, mean = 0,  sd = 0.7)
)
# Figure
ggplot() + 
  # Jeu de points
  geom_point(data = mod_l1fig, aes(x = x, y = y)) +
  # Ajustement du modèle
  geom_abline(slope = beta_1, intercept = beta_0) + 
  # Points y*
  geom_point(data = mod_l1fig_predict, aes(x = x, y = y), col = 'red') +
  # Distribution autour de y*
  geom_point(data = norm_dta, aes(x = x, y = y), col = 'red', alpha=0.02) +
  xlim(c(5, 15)) + ylim(c(2, 10))
```


## Hypothèses

- Indépendance des erreurs : $\mathrm{Cov}(\epsilon_i, \epsilon_j) = 0$.
Assurée par le design expérimental.

- Exogénéité : $X$n'est pas corrélé à $\Epsilon$.

- Homoscédasticité : la variance de l'erreur est constante sur l'étendue de $X$.

- Normalité des termes d'erreur : $\Epsilon \sim \mathcal{N}(0,\sigma^{2})$.


## Exemple

Générer les données du modèle.

Coefficients : 
```{r}
beta_0 <- 1
beta_1 <- 0.5
sigma <- 1
```

Tirage :
```{r}
n <- 100
x <- runif(n, min = 5, max = 15)
# Jeu de points
mod_l1 <- tibble(x, y = rnorm(n, mean = beta_0 + beta_1*x, sd = sigma))
```


## Estimation

Commencer par une figure.

```{r}
mod_l1 |> 
  ggplot(aes(x = x, y = y)) + geom_point() + geom_smooth(method = lm)
```


## Estimation

La fonction `lm()` du package *stats* estime le modèle et permet de tester les hypothèses.

```{r}
mod_l1_lm <- lm(y ~ x, data = mod_l1)
```

Syntaxe de la formule :

- variable expliquée à gauche, covariables à droite de `~`
- constante implicite `y ~ x` est identique à `y ~ 1 + x` alors que `y ~ 0 + x` force la constante à 0.
- possibilité de transformer les variables : `log(y) ~ I(x^2)` (Attention : `log(y) ~ x^2` est interprété comme l'interaction de `x` avec lui-même, c'est-à-dire `x`)


## Homoscédasticité et indépendance des erreurs

Graphique $\Epsilon \sim Y^\star$

```{r}
#| out.width: 60%
plot(mod_l1_lm, which = 1)
```

Les erreurs doivent être centrée sur 0 et uniformément réparties.


## Normalité des erreurs

Graphique quantile - quantile (`?qqplot`)

```{r}
#| out.width: 60%
plot(mod_l1_lm, which = 2)
```

La non-normalité des résidus implique la non-normalité des estimateurs des coefficients. 


## Test de normalité

Utiliser le test de Shapiro-Wilk :

```{r}
mod_l1_lm |> residuals() |> shapiro.test()
```
La p-value est la probabilité de se tromper en rejetant l'hypothèse nulle de normalité des données.
Attention : la puissance du test augmente avec la taille de l'échantillon (limité à 5000).


## Test de Kolmogorov-Smirnov

Teste l'hypothèse que deux échantillons sont issus de la même distribution :

```{r}
mod_l1_lm |> residuals() %>% ks.test(rnorm(length(.), 0, var(.)))
```

Plus général que Shapiro-Wilk.

$\to$ tester un tirage dans une loi uniforme contre une distribution normale.
Combien de valeurs faut-il pour rejeter H0 ?


## Effet de levier

```{r}
#| out.width: 60%
plot(mod_l1_lm, which = 5)
```
Les points avec fort effet de levier forte erreur ($\to$ grande distance de Cook) posent problème.


## Rectification des données

Affaire d'expérience.

- Éliminer les points (réellement) aberrants ;
- Transformer $Y$ si :
  - la relation n'est pas linéaire (ex.: quadratique) ;
  - l'erreur augmente avec $Y^\star$ ($\to$ racine carrée ou logarithme).
- Revoir les hypothèses à l'origine du modèle, le design expérimental...


## Interprétation des résultats : `summary`

```{r}
#| echo: false
summary(mod_l1_lm)
```

## Statistique F

La statistique F décrit la probabilité que le modèle n'explique rien.

Modèle nul: $Y = \bar{Y} = \beta_0$

```{r}
#| echo: false
# Jeu de points
mod_l1null <- tibble(
  x = mod_l1fig$x,
  y = rnorm(n_fig, mean = beta_0, sd = sigma)
)
# Points y*
x0 <- 7:13
mod_l1null_predict <- tibble(x = x0, y = beta_0)
# Distribution autour de y*. Le nombre de points doit être multiple de lengh(x0)
norm_dta <- tibble(
  y = rnorm(7000, mean = beta_0, sd = sigma), 
  x = x0 + dnorm(x = y- beta_0, mean = 0,  sd = 0.7)
)
# Figure
ggplot() + 
  # Jeu de points
  geom_point(data = mod_l1null, aes(x = x, y = y)) +
  # Ajustement du modèle
  geom_hline(yintercept = beta_0) + 
  # Points y*
  geom_point(data = mod_l1null_predict, aes(x = x, y = y), col = 'red') +
  # Distribution autour de y*
  geom_point(data = norm_dta, aes(x = x, y = y), col = 'red', alpha=0.02) +
  xlim(c(5, 15)) + ylim(c(-2, 3))
```


## R²

R² mesure la proportion de la variance de Y expliquée par le modèle : 
$$R^2 = \frac{\mathrm{Var}(Y^\star)}{\mathrm{Var}(Y)} = 1 - \frac{\sigma}{\mathrm{Var}(Y)}$$


$\to$ Que devient R² en doublant $\sigma$ ?
Estimer rapidement puis re-simuler le modèle pour vérifier.

R² ajusté pénalise le R² par le nombre de paramètres du modèle. 

Les degrés de liberté sont le nombre d'observations moins le nombre de paramètres moins 1.


## Estimation des coefficients

Les coefficients sont estimés par la méthode des moindres carrés : minimisation des écarts $$\sum(y_i - y_i^\star)^2$$.

Résultat identique à la maximisation de la vraisemblance $$\prod{f(\epsilon_i)}$$ où $f(\dot)$ est la densité de $\mathcal{N}(0,\sigma^{2})$.


## Estimation des coefficients

L'estimateur de chaque coefficient est sa valeur la plus probable.

L'estimateur est distribué normalement (quand $\Epsilon$ est normal) :

$$\hat{\beta}_1 \sim \mathcal{N}(`r format(mod_l1_lm$coefficients[2], digits = 3)`, \sigma_1^2)$$
où $\sigma_1$ est l'écart-type de l'estimateur.
`lm()` donne son erreur standard, c'est-à-dire $\sigma_1/\sqrt{n}$.

Un test de Student donne la probabilité de se tromper en affirmant que l'estimateur n'est pas nul.


## Synthèse 1/2

Un bon modèle a un grand R² et des petites p-values.

- R² diminue avec la variance de l'erreur ;
- L'écart-type des estimateur diminue comme $\sqrt{n}$.

Mais les deux dépendent du design expérimental.


## Design expérimental

Quadrupler l'effort d'échantillonnage divise par deux l'intervalle de confiance

```{r}
mod_l1x4 <- tibble(
  x = rnorm(n * 4, mean = 10, sd = 2), # x est calculé avant y 
  y = rnorm(n * 4, mean = beta_0 + beta_1 * x, sd =sigma) # y utilise x
)
mod_l1x4_lm <- lm(y ~ x, data = mod_l1x4)
summary(mod_l1x4_lm)$coefficients
```

Choix économique.


## Design expérimental

Retirer les valeurs intermédiaires de $X$ augmente le R² (*design factoriel*) alors que $\sigma$ ne change pas.

```{r}
mod_l1x4 |> 
  filter(x < 6 | x >14) %>% # pas |> pour "data = ."
  lm(y ~ x, data = .) |> 
  summary() |> 
  pluck("r.squared")
```

contre `r summary(mod_l1x4_lm)$r.squared` avec toutes les données.


## Design expérimental

Le R² d'un modèle avec des données individuelles est plus faible qu'avec des données agrégées.

$\to$ Estimer le modèle hauteur ~ diamètre des données Ventoux.

$\to$ Regrouper les données par espèce.

$\to$ Estimer le modèle à nouveau.


## Synthèse 2/2

Considérer R² et p-values en fonction du modèle :

- beaucoup de données individuelles $\to$ faible R² mais petites p-values pour montrer l'influence d'un facteur ;
- possibilité d'un très grand R² sans aucun coefficient significatif si peu de points ;
- un grand R² et des petites p-values permettent de faire des prédictions.


## Prédictions

`predict()` permet d'extrapoler le modèle.

```{r}
mod_l1_lm |> predict(newdata = data.frame(x = 5:10))
```


## Prédictions

Ajout des points sur la figure :

```{r}
#| out.width: 40%
# Estimation du modèle
mod_l1 |> 
  ggplot(aes(x, y)) + geom_point() + geom_smooth(method = lm) -> 
  mod_l1_ggplot
# Choix des x pour lesquels y est à prédire
mod_l1_predict <- data.frame(x = 5:10)
# Ajout des prédictions
mod_l1_predict$y <- predict(mod_l1_lm, newdata = mod_l1_predict)
# Ajout des points à la figure précédente
mod_l1_ggplot +
  geom_point(data = mod_l1_predict, aes(x = x, y = y), col = "red")
```

## Intervalles de confiance et de prédiction

La zone grisée de `geom_smooth` est l'intervalle de confiance de l'espérance de $Y|X$, c'est-à-dire de la moyenne des prédictions.
Il est bien plus étroit que l'intervalle de prédiction, qui correspond à 95% des prédictions :

```{r}
mod_l1_predict <- data.frame(
  x = seq(from = min(mod_l1$x), to = max(mod_l1$x), length.out = 50)
)
mod_l1_predict <- cbind(
  mod_l1_predict,
  predict(
    mod_l1_lm, 
    newdata = mod_l1_predict, 
    interval = "prediction"
  )
)
mod_l1_ggplot +
  geom_ribbon(
    data = mod_l1_predict, 
    aes(y = fit, ymin = lwr, ymax = upr),
    alpha = 0.3
  ) -> mod_l1_ggplot_predict
```

## Intervalles de confiance et de prédiction

```{r}
#| echo: false
mod_l1_ggplot_predict
```


# Régression linéaire multiple

## Théorie

Modèle linéaire multiple :
$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \Epsilon$$
$Y$ et $X_j$ sont des vecteurs : $X_1 = \{x_{i,1}\}$ est l'ensemble des valeurs du premier prédicteur (= variable explicative, variable exogène ou covariable).


## Représentation

Multidimensionnelle donc plus difficile.

Le dimension de $Y$ est égale au nombre de covariables moins 1: le modèle linéaire réduit la dimension des données.

Ajout d'un coefficient à l'exemple précédent :

```{r}
beta_0 <- 1
beta_1 <- 0.5
beta_2 <- 2
sigma <- 5
```

Tirage :
```{r}
n <- 100
x_1 <- runif(n, min = 5, max = 15)
x_2 <- runif(n, min = 0, max = 10)
# Jeu de points
mod_l2 <- tibble(
  x_1, x_2, 
  y = rnorm(n, mean = beta_0 + beta_1*x_1 + beta_2*x_2, sd = sigma)
)
```


## Représentation

```{r}
mod_l2_lm  <- lm(y ~ x_1 + x_2, data = mod_l2)
```


```{r}
#| echo: false
library("scatterplot3d")
# Création du graphique 3D
s3d <- scatterplot3d(
  # Position des points, 3 colonnes x, y et z
  mod_l2,
  # Type de graphique : points
  type = "p", 
  # Cercles
  pch = 19,
  # Couleurs des points sous le plan, les autres seront retracés
  color = "darkgrey",
  # Afficher le repère à z =0
  grid = TRUE, 
  # Ne pas afficher de boîte autour du graphique
  box = FALSE,
  # Marges
  mar = c(1, 1, 1, 1),
  # Angle de vision (0 = horizontal)
  angle = 60
)
# Quels points sont au-dessus du plan ?
is_over <- resid(mod_l2_lm) > 0
# Segments du point au plan
xyz_y <- s3d$xyz.convert(mod_l2$x_1, mod_l2$x_2, mod_l2$y)
xyz_ystar <- s3d$xyz.convert(mod_l2$x_1, mod_l2$x_2, fitted(mod_l2_lm))
# Tracé des segments
segments(
  x0 = xyz_y$x, y0 = xyz_y$y, 
  x1 = xyz_ystar$x, y1 = xyz_ystar$y, 
  col = c("orange", "red")[1 + is_over],
  # Pointillé ou plein selon la position
  lwd = 1.5
)
# Redessiner les points en deux temps : sous le plan...
s3d$points3d(
  x = mod_l2$x_1[!is_over], y = mod_l2$x_2[!is_over], z = mod_l2$y[!is_over], 
  pch = 19, 
  col = "darkgrey"
)
# Tracer le plan de régression
s3d$plane3d(
  mod_l2_lm, 
  draw_polygon = TRUE, 
  draw_lines = TRUE
)
# ... puis au-dessus du plan
s3d$points3d(
  x = mod_l2$x_1[is_over], y = mod_l2$x_2[is_over], z = mod_l2$y[is_over], 
  pch = 19, 
  col = "black"
)
```

## Hypothèses

En plus des précédentes :

- Non colinéarité des covariables.

Si une des covariables est une combinaison linéaire des autres, le modèle ne peut pas être estimé.

En pratique, les covariables doivent être aussi peu corrélées que possible.


## Interactions

On peut tester l'effet de l'interaction de deux variables :

```{r}
lm(y ~ x_1 + x_2 + x_1:x_2, data = mod_l2) |> # Identique à x_1*x_2
  summary()
```


## Standardisation

Il est possible de standardiser toutes les variables pour comparer les effets des covariables.


```{r}
lm(scale(y) ~ 0 + scale(x_1) + scale(x_2), data = mod_l2) |> summary()
```


## Standardisation

Dans un modèle linéaire standardisé simple, le coefficient égale la corrélation.

```{r}
lm(scale(y) ~ scale(x), data = mod_l1) |> summary()
```


## Test de la corrélation

La significativité de la corrélation entre deux variables est celle du coefficient de la régression standardisée.

Plus simplement :

```{r}
with(mod_l1, cor.test(x, y))
```


# Régression sur les rangs

## Théorie

Si les résidus ne sont pas normaux, il est possible de faire la régression sur les rangs des variables :

- régression simple : revient à tester la corrélation de Spearman.


## Exemple

Modèle univarié :

```{r}
lm(rank(y) ~ rank(x), data = mod_l1)|> summary()
```
## Exemple

Test de la corrélation :

```{r}
with(mod_l1, cor.test(x, y, method = "spearman"))
```
Les modèles sont équivalents et donnent la même p-value.


# Transformation de variables

## Principe

Le modèle linéaire permet de traiter des modèles non linéaires en transformant les variables.

Exemple : le volume $V$ d'un arbre est lié à son diamètre $D$ à la puissance $\beta_1$

$\to$ Modèle : $$\ln(V) = \beta_0 + \beta_1 \ln(D) + \Epsilon$$


## Pratique

Dans la formule de `lm()`, certains opérateurs sont compris, d'autres non : essayer.

```{r}
# Données
n <- 100
D <- runif(n, min = 10, max = 50)
V <- exp(2.5 * log(D)) + rnorm(n)
# Modèle
lm(log(V) ~ log(D))
```

## Pratique

Autres écritures :

- `I(D^2.5)` : le contenu de `I()` peut être n'importe quel calcul valide
```{r}
#| eval: false
lm(V ~ I(D^2.5))
```
- `poly(D, degree = 3)` : toutes les puissances de D jusqu'à 3.
```{r}
#| eval: false
lm(V ~ poly(D, degree = 3))
```

## Pourquoi transformer

Modèle mécaniste : la relation entre volume et diamètre est à la puissance 3 si l'arbre est un cylindre.

Contrôle de la variance : dans certains cas, la variance augmente avec $Y^\star$.
On peut essayer de régresser $\sqrt{Y}$  ou $\ln(Y)$.

Mais on ne doit pas tenter à l'aveugle toutes les transformations possibles : voir le problème des tests multiples dans le cours sur l'Anova. 



# Ancova

## Théorie

Modèle de régression multiple avec des covariables catégorielles, codées sous forme d'indicatrices (autant d'indicatrices que de modalités - 1).

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \Epsilon$$

Exemple du Ventoux :

- $Y$ est la hauteur des arbres ;
- $X_1$ est leur diamètre ;
- L'espèce est codée par une variable indicatrice, par exemple $X_2 = \mathbb{1}('Cedre')$.


## Exemple

```{r}
#| out.width: 70%
ventoux |> 
  ggplot(aes(x = diametre, y = hauteur, color = espece)) +
  geom_point() +
  geom_smooth(method = "lm")
```

## Estimation

La figure représente *deux régressions séparées* : les pentes pourraient être différentes.
Une Ancova est donc appropriée.

`lm` crée automatiquement des indicatrices pour les variables catégorielles.

```{r}
(ventoux_lm <- lm(hauteur ~ diametre + espece, data = ventoux))
```

Ici, l'indicatrice vaut 1 pour les pins, 0 pour les cèdres.


## Représentation

La figure doit être construite manuellement

```{r}
ventoux |> 
  bind_cols(predict(ventoux_lm, interval = "confidence")) |> 
  ggplot(aes(x = diametre, color = espece)) +
    geom_point(aes(y = hauteur)) +
    geom_line(aes(y = fit)) +
    geom_ribbon(aes(y = fit, ymin = lwr, ymax = upr),  alpha = 0.3)
```



# Sélection de modèle

## Théorie

Un modèle avec trop peu de covariables est sous-ajusté.
Il explique mal $Y$, avec une erreur qui ne diminue pas quand le nombre d'observations augmente : on parle de *biais*.
Cas extrême : $Y = \beta_0 + \Epsilon$

Un modèle avec trop de covariables est sur-ajusté.
Avec le même nombre d'observations qu'un modèle plus simple, ses coefficients sont une *variance* plus grande.
Cas extrême : $Y = \beta_0 +  \sum_{i=1}^{n-1}{\beta_i X_i} + \Epsilon$


## Théorie

Beaucoup de méthodes pour choisir le "meilleur" modèle, solide support théorique.

Critère d'Information d'Akaike (AIC) : $2K -2 \ln(L)$ où $L$ est la vraisemblance et $K$ le nombre de paramètres (les $\beta_i$ et $\sigma$).

Critère AICc pour de petits échantillons : 
$$2K \frac{n}{n -K -1} -2 \ln(L) $$

## Exemple

Faut-il ajouter le paramètre espèce au modèle Ventoux ?

On peut calculer l'AICc d'un modèle

```{r}
library("AICcmodavg")
lm(hauteur ~ diametre, data = ventoux) |> 
  AICc()
```
Pour la comparaison, une liste de modèles est nécessaire :

```{r}
ventoux_lm_list <- list(
  nul = lm(hauteur ~ 1, data = ventoux),
  diametre = lm(hauteur ~ diametre, data = ventoux),
  diamespece = lm(hauteur ~ diametre + espece, data = ventoux),
  complet = lm(hauteur ~ diametre*espece, data = ventoux)
)
```

## Exemple

```{r}
aictab(ventoux_lm_list)
```

Le meilleur modèle est celui avec l'espèce mais sans l'interaction.

Les poids permettent des prédictions multi-modèles.

## Exemple

Prédiction pour de nouvelles valeurs:

```{r}
ventoux_nouveau <- data.frame(
  diametre = c(20, 50),
  espece = c("Pin", "Cèdre")
)
modavgPred(ventoux_lm_list, newdata = ventoux_nouveau)
```

## Sélection systématique

Sélection (*backward*) :

- Estimer le modèle complet,
- Retirer la covariable qui fait le plus diminuer l'AIC jusqu'à ce qu'il ne diminue plus

Élimination (*forward*) :

- Estimer le modèle nul,
- Ajouter la covariable qui fait le plus diminuer l'AIC jusqu'à ce qu'il ne diminue plus.

Mixte (*stepwise*): 

- Élimination puis sélection successives.


## Exemple

Critère AIC, pas AICc :

```{r}
library("MASS")
stepAIC(lm(hauteur ~ diametre * espece, data = ventoux))
```


<!-- Styles for HTML slides -->
<style>
  /* Allow long bibliography */
  .forceBreak { -webkit-column-break-after: always; break-after: column; }
  slides > slide { overflow: scroll; }
  slides > slide:not(.nobackground):after { content: ''; }

  /* First page logo size */
  .gdbar img {
    width: 200px !important;
    height: 55px !important;
    margin: 8px 8px;
  }
  .gdbar {
    width: 250px !important;
    height: 70px !important;
  }
  
  /* No logo on slides */
  slides > slide:not(.nobackground):before {
    display:none
  }
</style>
